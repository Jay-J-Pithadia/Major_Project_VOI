{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a0uVtNFVOMwa"},"outputs":[],"source":["%%capture\n","!pip install gtts ultralytics supervision pydub googletrans==4.0.0-rc1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQU-aeNfRtjU"},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image, Audio\n","from google.colab.output import eval_js\n","from google.colab.patches import cv2_imshow\n","from base64 import b64decode, b64encode\n","import cv2, PIL, io, os, html, time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import torch\n","from gtts import gTTS\n","from ultralytics import YOLO\n","from googletrans import Translator\n","from supervision import Detections, LabelAnnotator, BoundingBoxAnnotator\n","import soundfile as sf\n","import librosa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhpAeMptSeVv"},"outputs":[],"source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGB')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8utlBGZSRnxy"},"outputs":[],"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","\n","    var pendingResolve = null;\n","    var shutdown = false;\n","\n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","\n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","\n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","\n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","\n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","\n","      const instruction = document.createElement('div');\n","      instruction.innerHTML =\n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","\n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","\n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","\n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","\n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","\n","      return {'create': preShow - preCreate,\n","              'show': preCapture - preShow,\n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","\n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYT9kFczOMn6"},"outputs":[],"source":["class ObjectDetection:\n","    def __init__(self, capture_index):\n","        self.capture_index = capture_index\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        print(\"Using Device: \", self.device)\n","        self.model = self.load_model()\n","        self.CLASS_NAMES_DICT = self.model.model.names\n","        self.box_annotator = BoundingBoxAnnotator()\n","        self.label_annotator = LabelAnnotator()\n","        self.KNOWN_DISTANCE = 16 #INCHES\n","        self.MOBILE_WIDTH = 3.0 #INCHES\n","        model = YOLO(\"yolov8l.pt\", task='detect')\n","        res = model.predict('/Ref_images/person_mobile1.jpg', verbose=False)\n","        x1, y1, x2, y2 = res[0][1].boxes.xyxy.cpu().numpy()[0]\n","        # mobile width in ref frame (in pixels)\n","        self.mobile_width_in_rf = x2 - x1\n","\n","\n","    def load_model(self):\n","        model = YOLO(\"yolov8l.pt\", task='detect')\n","        model.fuse()\n","        return model\n","\n","    def predict(self, frame):\n","        results = self.model.predict(frame, verbose=False)\n","        return results\n","\n","    def focal_length_finder (self, measured_distance, real_width, width_in_rf):\n","        focal_length = (width_in_rf * measured_distance) / real_width\n","        return focal_length\n","\n","    # distance finder function\n","    def distance_finder(self, focal_length, real_object_width, width_in_frmae):\n","        distance = (real_object_width * focal_length) / width_in_frmae\n","        return int(round(distance,0))\n","\n","    def mobile_width_in_frame(self, detections):\n","        for xyxy, mask, confidence, class_id, tracker_id, data in detections:\n","            if class_id==67:\n","                x1, y1, x2, y2 = xyxy\n","                width = x2 - x1\n","                return width\n","\n","    def play_audio(self, text, ch):\n","        if ch==1:\n","            tts = gTTS(text=text, lang='en', tld=\"us\")\n","            print(text)\n","        elif ch==2:\n","            translator = Translator()\n","            translated = translator.translate(text, src='en', dest='hi')\n","            tts = gTTS(text=translated.text, lang='hi', tld=\"us\")\n","            print(translated.text)\n","        else:\n","            translator = Translator()\n","            translated = translator.translate(text, src='en', dest='mr')\n","            tts = gTTS(text=translated.text, lang='mr', tld=\"us\")\n","            print(translated.text)\n","\n","        tts.save('temp_audio.wav')\n","        audio, sr = librosa.load(\"temp_audio.wav\")\n","        # Increase the speed by 1.5x\n","        faster_audio = librosa.effects.time_stretch(audio, rate=1.5)\n","        sf.write(\"temp_audio.wav\", faster_audio, sr)\n","        display(Audio('temp_audio.wav', autoplay=True))\n","        time.sleep(2)\n","\n","    def plot_bboxes(self, results, frame, ch):\n","        xyxys = []\n","        confidences = []\n","        class_ids = []\n","\n","        # Extract detections if conf > 0.8\n","        for result in results[0]:\n","            class_id = result.boxes.cls.cpu().numpy().astype(int)\n","            confidence = result.boxes.conf.cpu().numpy()\n","            if confidence > 0.75:\n","                xyxy = result.boxes.xyxy.cpu().numpy()\n","                xyxys.append(xyxy.reshape(-1, 4))\n","                confidences.append(confidence)\n","                class_ids.append(class_id)\n","\n","        if xyxys:\n","            # Setup detections for visualization\n","            detections = Detections(\n","                xyxy=np.concatenate(xyxys),\n","                confidence=np.concatenate(confidences),\n","                class_id=np.concatenate(class_ids),\n","            )\n","\n","            # Logic to check if mobile detected\n","            if any([True if class_id==67 else False for xyxy, mask, confidence, class_id, tracker_id, data in detections]):\n","                focal_mobile = self.focal_length_finder(self.KNOWN_DISTANCE, self.MOBILE_WIDTH, self.mobile_width_in_rf)\n","                width_in_frame = self.mobile_width_in_frame(detections)\n","                distance = self.distance_finder(focal_mobile, self.MOBILE_WIDTH, width_in_frame)\n","                # Format custom labels\n","                self.labels = [f\"Mobile at Distance: {distance} inches\" if class_id==67 else f'{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}' for xyxy, mask, confidence, class_id, tracker_id, data in detections]\n","\n","                txt = f'Mobile is {distance} inches away'\n","                self.play_audio(txt, ch)\n","\n","                # Annotate and display frame\n","                frame = self.box_annotator.annotate(scene=frame, detections=detections)\n","                frame = self.label_annotator.annotate(scene=frame, detections=detections, labels=self.labels)\n","                return frame\n","\n","            # Format custom labels\n","            self.labels = [f\"{self.CLASS_NAMES_DICT[class_id]} {confidence:0.2f}\" for xyxy, mask, confidence, class_id, tracker_id, data in detections]\n","\n","            # Play audio sound\n","            if self.labels:\n","                if len(self.labels)==1:\n","                    for label in self.labels:\n","                        item = label.split()\n","                        if len(item)==3:\n","                            txt = f\"There is a {item[0]} {item[1]} ahead\"\n","                            self.play_audio(txt, ch)\n","                        elif len(item)==2:\n","                            txt = f\"There is a {item[0]} ahead\"\n","                            self.play_audio(txt, ch)\n","                else:\n","                    txt = f'There are {len(self.labels)} objects: '\n","                    for label in self.labels[:-1]:\n","                        item = label.split()\n","                        if len(item)==3:\n","                            txt += f\"{item[0]} {item[1]}, \"\n","                        elif len(item)==2:\n","                            txt += f\"{item[0]}, \"\n","                    # Adding last item\n","                    last_item = self.labels[-1].split()\n","                    if len(last_item)==3:\n","                        txt += f\"and {last_item[0]} {last_item[1]}\"\n","                    elif len(last_item)==2:\n","                        txt += f\"and {last_item[0]}\"\n","                    self.play_audio(txt, ch)\n","\n","            # Annotate and display frame\n","            frame = self.box_annotator.annotate(scene=frame, detections=detections)\n","            frame = self.label_annotator.annotate(scene=frame, detections=detections, labels=self.labels)\n","        return frame\n","\n","    def __call__(self):\n","        print(\"Choose Language...\\n  1. For English\\n  2. For Hindi\\n  3. For Marathi\\n\")\n","        while True:\n","            try:\n","                ch = int(input('Enter Here (1, 2 or 3): '))\n","                if ch not in [1, 2, 3]:\n","                    print(\"Invalid input. Please enter 1, 2, or 3.\")\n","                else:\n","                    break\n","            except ValueError:\n","                print(\"Invalid input. Please enter a valid number.\")\n","\n","        # start streaming video from webcam\n","        video_stream()\n","        # label for video\n","        label_html = 'Capturing...'\n","        # initialze bounding box to empty\n","        bbox = ''\n","        count = 0\n","        while True:\n","            js_reply = video_frame(label_html, bbox)\n","            if not js_reply:\n","                break\n","\n","            # convert JS response to OpenCV Image\n","            frame = js_to_image(js_reply[\"img\"])\n","            results = self.predict(frame)\n","            frame = self.plot_bboxes(results, frame, ch)\n","            cv2_imshow(frame)\n","            if cv2.waitKey(5) & 0xFF == 27:\n","                break\n","\n","if __name__==\"__main__\":\n","    detector = ObjectDetection(capture_index=0)\n","    detector()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-FBCG93wMfo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OULVOnWSy8xi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTMXRDNcy9J1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"O9NY1fNZwNDo"},"source":["### Only for testing purpose"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_usIggekJob6"},"outputs":[],"source":["model = YOLO(\"/content/drive/MyDrive/Major project/yolov8l.pt\", task='detect')\n","res = model.predict('/content/drive/MyDrive/Major project/mobile1.jpg', verbose=False)\n","# for result in res[0][1]\n","class_id = res[0][1].boxes.cls.cpu().numpy().astype(int)\n","confidence = res[0][1].boxes.conf.cpu().numpy()\n","x1, y1, x2, y2 = res[0][1].boxes.xyxy.cpu().numpy()[0]\n","width = x2 - x1\n","print(class_id[0], confidence[0])\n","print(width)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaEvhV9sJgjR"},"outputs":[],"source":["model = YOLO(\"/content/drive/MyDrive/Major project/yolov8l.pt\", task='detect')\n","res = model.predict('/content/drive/MyDrive/Major project/mobile1.jpg', verbose=False)\n","xyxys = []\n","confidences = []\n","class_ids = []\n","for result in res[0]:\n","    class_id = result.boxes.cls.cpu().numpy().astype(int)\n","    confidence = result.boxes.conf.cpu().numpy()\n","    if confidence > 0.75:\n","        xyxy = result.boxes.xyxy.cpu().numpy()\n","        xyxys.append(xyxy.reshape(-1, 4))\n","        confidences.append(confidence)\n","        class_ids.append(class_id)\n","\n","if xyxys:\n","    # Setup detections for visualization\n","    detections = Detections(\n","        xyxy=np.concatenate(xyxys),\n","        confidence=np.concatenate(confidences),\n","        class_id=np.concatenate(class_ids),\n","    )\n","    for xyxy, mask, confidence, class_id, tracker_id, data in detections:\n","      if class_id==67:\n","        print(xyxy)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
